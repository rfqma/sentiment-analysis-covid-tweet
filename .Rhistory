knitr::opts_chunk$set(echo = TRUE)
library(vroom)
library(here)
library(sentimentr)
library(tidytext)
library(tidyr)
library(textclean)
library(tibble)
library(tm)
rawData = read.csv("tweet-pemerintah-dan-covid.csv", header = T)
tweets = rawData$tweet
tweets.text = Corpus(VectorSource(tweets))
removeURL <- function(x) gsub("http[^[:space:]]*", "",x)
clean <- tm_map(tweets.text, removeURL)
removeNL <- function(y) gsub("\n", " ", y)
clean <- tm_map(clean, removeNL)
removepipe <- function(z) gsub("<[^>]+>", "", z)
clean <- tm_map(clean, removepipe)
remove.mention <- function(z) gsub("@\\S+", "", z)
clean <- tm_map(clean, remove.mention)
remove.hashtag <- function(z) gsub("#\\S+", "", z)
clean <- tm_map(clean, remove.hashtag)
removeamp <- function(y) gsub("&amp;", "", y)
clean <- tm_map(clean, removeamp)
removetitik3 <- function(y) gsub("[[:punct:]]", "", y)
clean <- tm_map(clean, removetitik3)
remove.all <- function(xy) gsub("[^[:alpha:][:space:]]*", "", xy)
clean <- tm_map(clean,remove.all)
clean <- tm_map(clean, tolower)
## Menghilangkan Extra Whitespace (Spasi)
clean <- tm_map(clean, stripWhitespace)
## Load Stopword-ID
stopwordID <- "ID-stopwords.txt"
## Membaca Stopword-ID per-baris
cStopwordID <- readLines(stopwordID)
## Load Slangword (Bahasa Gaul)
slang <- read.csv("Slangword.csv", header = T)
old_slang <- as.character(slang$old)
new_slang <- as.character(slang$new)
## Load Stemming (Kata Imbuhan)
stemm <- read.csv("Stemming.csv", header = T)
old_stemm <- as.character(stemm$old)
new_stemm <- as.character(stemm$new)
## Load Lemmatization (Pengelompokan Infleksi Kata)
lemma <- read.csv("Lemmatization.csv", header = T)
old_lemma <- as.character(stemm$old)
new_lemma <- as.character(stemm$new)
stemmword <- function(x) Reduce(function(x,r) gsub(stemm$old[r],stemm$new[r],x,fixed=T),
seq_len(nrow(stemm)),x)
clean <- tm_map(clean,stemmword)
slangword <- function(x) Reduce(function(x,r) gsub(slang$old[r],slang$new[r],x,fixed=T),
seq_len(nrow(slang)),x)
clean <- tm_map(clean,slangword)
lemmatization <- function(x) Reduce(function(x,r) gsub(lemma$old[r],lemma$new[r],x,fixed=T),
seq_len(nrow(lemma)),x)
clean <- tm_map(clean,lemmatization)
clean <- tm_map(clean, removeWords, cStopwordID)
writeLines(strwrap(clean[[2]]$content, 100))
dataframe = data.frame(text = unlist(sapply(clean, `[`)), stringsAsFactors = F)
View(dataframe)
write.csv(dataframe, file = 'dataTweetBersih.csv')
library(e1071)
library(caret)
library(syuzhet)
data <- read.csv("dataTweetBersih.csv", stringsAsFactors = FALSE)
## Deklarasi Variabel Teks Kolom (Column Text) Menjadi Char
tweetss <- as.character(data$text)
## Memanggil NRC Sentiment Dictionary untuk menghitung berbagai emosi
s <- get_nrc_sentiment(tweetss)
tweetss_combine <- cbind(data$text,s)
par(mar=rep(3,4))
a <- barplot(colSums(s), col = rainbow(10), ylab = 'count', main = 'Sentiment Analisis')
iki_ba <- a
library(wordcloud)  # Library Wordcloud
library(tm)         # Library Penggunaan Corpus dalam Cleaning Data
library(RTextTools) # Library Penggunaan Corpus dalam Cleaning Data
library(e1071)      # Library yang di dalamnya terdapat sebuah Algoritma Naive Bayes
library(dplyr)      # Library yang di dalamnya terdapat sebuah Algoritma Naive Bayes
library(caret)      # Library yang di dalamnya terdapat sebuah Algoritma Naive Bayes
df <- read.csv("dataTweetBersih.csv", stringsAsFactors = FALSE)
glimpse(df)
set.seed(20)
df<-df[sample(nrow(df)),]
df<-df[sample(nrow(df)),]
glimpse(df)
corpus<-Corpus(VectorSource(df$text))
corpus
inspect(corpus[1:10])
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords,stopwords(kind="en"))%>%
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train<-df[1:50,]
df.test<-df[51:100,]
dtm.train<-dtm[1:50,]
dtm.test<-dtm[51:100,]
corpus.clean.train<-corpus.clean[1:50]
corpus.clean.test<-corpus.clean[51:100]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
#dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
convert_count <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,1,convert_count)
wordcloud(corpus.clean,min.freq = 4,max.words=100,random.order=F,colors=brewer.pal(8,"Dark2"))
library(shiny)
ui <- fluidPage(
## Judul Aplikasi
titlePanel("Analisis Sentimen Terhadap Tweet Masyarakat Indonesia Tentang Covid dan Pemerintah"),
mainPanel(
##textOutput("selected_var"),
##plotOutput("asPlot"),
tabsetPanel(type = "tabs",
tabPanel("Data Asli", DT::dataTableOutput('tbl')),
tabPanel("Data Bersih", DT::dataTableOutput('tbl2')),
tabPanel("Scatterplot", plotOutput("asPlot")),
tabPanel("Wordcloud", plotOutput("wordcl"))
)
)
)
library(wordcloud)  # Library Wordcloud
library(tm)         # Library Penggunaan Corpus dalam Cleaning Data
library(RTextTools) # Library Penggunaan Corpus dalam Cleaning Data
library(e1071)      # Library yang di dalamnya terdapat sebuah Algoritma Naive Bayes
library(dplyr)      # Library yang di dalamnya terdapat sebuah Algoritma Naive Bayes
library(caret)      # Library yang di dalamnya terdapat sebuah Algoritma Naive Bayes
df <- read.csv("dataTweetBersih.csv", stringsAsFactors = FALSE)
glimpse(df)
library(shiny)
ui <- fluidPage(
## Judul Aplikasi
titlePanel("Analisis Sentimen Terhadap Tweet Masyarakat Indonesia Tentang Covid dan Pemerintah"),
mainPanel(
##textOutput("selected_var"),
##plotOutput("asPlot"),
tabsetPanel(type = "tabs",
tabPanel("Data Asli", DT::dataTableOutput('tbl')),
tabPanel("Data Bersih", DT::dataTableOutput('tbl2')),
tabPanel("Scatterplot", plotOutput("asPlot")),
tabPanel("Wordcloud", plotOutput("wordcl"))
)
)
)
## Membuat fungsi input, output, session
## Di dalam badan fungsi berisi seluruh kode pemrosesan data
## Membuat input dan menampilkan hasil pada output
server <- function(input, output, session) {
as_data <- reactive({
input$Update
isolate({
withProgress({
setProgress(message = "Processing analisis...")
as_file <- input$as
if(!is.null(as_file)){
as_text <- readLines(as_file$datapath)
}
else
{
as_text <- "A Barplot is an immage made of words that..."
}
})
})
})
barplot_rep <- repeatable(barplot)
# Menampilkan data asli
output$tbl = DT::renderDataTable({
DT::datatable(rawData, options = list(lengthchange = FALSE))
})
# Menampilkan Data Bersih
output$tbl2 = DT::renderDataTable({
DT::datatable(data, options = list(lengthchange = FALSE))
})
# Menampilkan Grafik Sentimen
output$asPlot <- renderPlot({ withProgress({
setProgress(message = "Creating barplot...")
barplot(colSums(s),col = rainbow(10),ylab = 'count',main = 'Sentiment Analysis')
})
})
# Menampilkan Wordcloud
output$wordcl <- renderPlot({
wordcloud(corpus.clean,min.freq = 4,max.words=100,random.order=F,colors=brewer.pal(8,"Dark2"))
})
}
shinyApp(ui = ui, server = server, options = list(height = "600px"))
knitr::opts_chunk$set(echo = TRUE)
df <- read.csv("dataTweetBersih.csv", stringsAsFactors = FALSE)
glimpse(df)
library(wordcloud)  # Library Wordcloud
library(tm)         # Library Penggunaan Corpus dalam Cleaning Data
library(RTextTools) # Library Penggunaan Corpus dalam Cleaning Data
library(e1071)      # Library yang di dalamnya terdapat sebuah Algoritma Naive Bayes
library(dplyr)      # Library yang di dalamnya terdapat sebuah Algoritma Naive Bayes
library(caret)      # Library yang di dalamnya terdapat sebuah Algoritma Naive Bayes
df <- read.csv("dataTweetBersih.csv", stringsAsFactors = FALSE)
glimpse(df)
# Mengatur seed generator bilangan acak R, yang berguna untuk membuat simulasi atau obyek acak yang dapat direproduksi
set.seed(20)
df<-df[sample(nrow(df)),]
df<-df[sample(nrow(df)),]
glimpse(df)
corpus<-Corpus(VectorSource(df$text))
corpus
inspect(corpus[1:10])
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords,stopwords(kind="en"))%>%
tm_map(stripWhitespace)
# Train dan Test Data
dtm<-DocumentTermMatrix(corpus.clean) # Naive Bayes
inspect(dtm[1:10,1:20])
df.train<-df[1:50,]
df.test<-df[51:100,]
dtm.train<-dtm[1:50,]
dtm.test<-dtm[51:100,]
corpus.clean.train<-corpus.clean[1:50]
corpus.clean.test<-corpus.clean[51:100]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
convert_count <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,1,convert_count)
#Training
classifier <- naiveBayes(trainNB, df.train$klasifikasi, laplace = 1)
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords,stopwords(kind="en"))%>%
tm_map(stripWhitespace)
# Train dan Test Data
dtm<-DocumentTermMatrix(corpus.clean) # Naive Bayes
inspect(dtm[1:10,1:20])
df.train<-df[1:340,]
df.test<-df[51:100,]
dtm.train<-dtm[1:50,]
dtm.test<-dtm[51:100,]
corpus.clean.train<-corpus.clean[1:50]
corpus.clean.test<-corpus.clean[51:100]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
convert_count <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,1,convert_count)
#Training
classifier <- naiveBayes(trainNB, df.train$klasifikasi, laplace = 1)
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords,stopwords(kind="en"))%>%
tm_map(stripWhitespace)
# Train dan Test Data
dtm<-DocumentTermMatrix(corpus.clean) # Naive Bayes
inspect(dtm[1:10,1:20])
df.train<-df[1:340,]
df.test<-df[51:100,]
dtm.train<-dtm[1:50,]
dtm.test<-dtm[51:100,]
corpus.clean.train<-corpus.clean[1:50]
corpus.clean.test<-corpus.clean[51:100]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
convert_count <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,1,convert_count)
df2 <- df.train$klasifikasi
#Training
classifier <- naiveBayes(trainNB, df2, laplace = 1)
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords,stopwords(kind="en"))%>%
tm_map(stripWhitespace)
# Train dan Test Data
dtm<-DocumentTermMatrix(corpus.clean) # Naive Bayes
inspect(dtm[1:10,1:20])
df.train<-df[1:37000,]
df.test<-df[51:100,]
dtm.train<-dtm[1:50,]
dtm.test<-dtm[51:100,]
corpus.clean.train<-corpus.clean[1:50]
corpus.clean.test<-corpus.clean[51:100]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
convert_count <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,1,convert_count)
#Training
classifier <- naiveBayes(trainNB, df.train$klasifikasi, laplace = 1)
{r}
{r}
require(corpus)
data.frame <- read.csv("dataLabelTweet.csv",stringsAsFactors = F)
data.frame$klasifikasi <- factor(data.frame$klasifikasi)
glimpse(data.frame)
set.seed(20)
data.frame<-data.frame[sample(nrow(data.frame)),]
data.frame<-data.frame[sample(nrow(data.frame)),]
glimpse(data.frame)
corpus<-Corpus(VectorSource(data.frame$text))
corpus
inspect(corpus[1:10])
#fungsinya untuk membersihkan data data yang tidak dibutuhkan
corpus.clean<-corpus %>%
tm_map(content_transformer(tolower)) %>% #digunakan untuk mengubah huruf besar dari string menjadi string huruf kecil
tm_map(removePunctuation)%>% #menghapus tanda baca
tm_map(removeNumbers)%>% #menghapus nomor
tm_map(removeWords,stopwords(kind="en"))%>% #menghapus stopwords
tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train<-data.frame[1:340,]
df.test<-data.frame[341:680,]
dtm.train<-dtm[1:340,]
dtm.test<-dtm[341:680,]
corpus.clean.train<-corpus.clean[1:340]
corpus.clean.test<-corpus.clean[341:680]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
#Boolan Naive Bayes
convert_count <- function(x){
y<-ifelse(x>0,1,0)
y<-factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
#Naive Bayes Model
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,2,convert_count)
#Training
classifier <- naiveBayes(trainNB, df.train$klasifikasi, laplace = 1)
#Use the NB classifier we built to make predictions on the test set
pred <- predict(classifier, testNB)
#Create a truth table by tabulating the predicted class labels with the actual predicted class labels with the actual class labels
NB_table=table("Prediction"= pred, "Actual" = df.test$klasifikasi)
NB_table
#confussion Matrix
conf.matNB <- confusionMatrix(pred, df.test$klasifikasi)
conf.matNB
